#!/usr/bin/env python3
"""
Manchester Academic Phrasebank Scraper
ä» University of Manchester å®˜æ–¹ç½‘ç«™çˆ¬å–å­¦æœ¯çŸ­è¯­åº“

å®˜æ–¹ç½‘ç«™: https://www.phrasebank.manchester.ac.uk/
æ•°æ®è®¸å¯: Educational Use (æ•™è‚²ç”¨é€”å…è´¹)
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from typing import List, Dict
import time

BASE_URL = "https://www.phrasebank.manchester.ac.uk"

# è®ºæ–‡å„éƒ¨åˆ†çš„ URL
SECTIONS = {
    "introduction": f"{BASE_URL}/introducing-work/",
    "methods": f"{BASE_URL}/describing-methods/",
    "results": f"{BASE_URL}/reporting-results/",
    "discussion": f"{BASE_URL}/discussing-findings/",
    "conclusion": f"{BASE_URL}/writing-conclusions/",
}

def scrape_section(url: str, section_name: str) -> List[Dict]:
    """
    çˆ¬å–æŸä¸ªéƒ¨åˆ†çš„å­¦æœ¯çŸ­è¯­

    Args:
        url: éƒ¨åˆ†çš„ URL
        section_name: éƒ¨åˆ†åç§° (introduction, methods, etc.)

    Returns:
        çŸ­è¯­åˆ—è¡¨
    """
    print(f"ğŸ“– Scraping {section_name} from {url}...")

    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        phrases = []
        phrase_id = 0

        # æŸ¥æ‰¾æ‰€æœ‰çŸ­è¯­ (é€šå¸¸åœ¨ <li> æˆ– <p> æ ‡ç­¾ä¸­)
        # Manchester Phrasebank çš„ç»“æ„å¯èƒ½éœ€è¦æ ¹æ®å®é™…ç½‘é¡µè°ƒæ•´
        content_area = soup.find('div', class_='entry-content') or soup.find('main')

        if not content_area:
            print(f"âš ï¸ Could not find content area in {section_name}")
            return []

        # æå–æ ‡é¢˜ä¸‹çš„çŸ­è¯­
        current_subsection = "general"

        for elem in content_area.find_all(['h2', 'h3', 'h4', 'ul', 'li', 'p']):
            if elem.name in ['h2', 'h3', 'h4']:
                # è¿™æ˜¯å­æ ‡é¢˜
                current_subsection = elem.get_text().strip().lower()
                current_subsection = re.sub(r'[^a-z0-9]+', '_', current_subsection)
                print(f"  ğŸ“‚ Subsection: {current_subsection}")

            elif elem.name == 'li':
                # è¿™æ˜¯ä¸€ä¸ªçŸ­è¯­
                phrase_text = elem.get_text().strip()

                # è·³è¿‡ç©ºæ–‡æœ¬
                if not phrase_text or len(phrase_text) < 10:
                    continue

                # è¯„ä¼°å­¦æœ¯åº¦ (åŸºäºç‰¹å¾)
                academic_score = evaluate_academic_score(phrase_text)

                # è¯„ä¼°ä½¿ç”¨é¢‘ç‡ (åŸºäºçŸ­è¯­ç‰¹å¾)
                frequency = evaluate_frequency(phrase_text)

                phrase_id += 1
                phrases.append({
                    "id": f"{section_name}_{current_subsection}_{phrase_id}",
                    "phrase": phrase_text,
                    "usage": "",  # å¯ä»¥æ‰‹åŠ¨è¡¥å……æˆ–ä½¿ç”¨ AI ç”Ÿæˆ
                    "academicScore": academic_score,
                    "frequency": frequency,
                    "examples": [phrase_text],  # çŸ­è¯­æœ¬èº«ä½œä¸ºç¤ºä¾‹
                    "section": section_name,
                    "subsection": current_subsection
                })

        print(f"âœ… Found {len(phrases)} phrases in {section_name}")
        return phrases

    except Exception as e:
        print(f"âŒ Error scraping {section_name}: {e}")
        return []

def evaluate_academic_score(phrase: str) -> float:
    """
    åŸºäºè¯­è¨€ç‰¹å¾è¯„ä¼°å­¦æœ¯åº¦è¯„åˆ†

    ç‰¹å¾:
    - ä½¿ç”¨è¢«åŠ¨è¯­æ€ (was/were/been + past participle)
    - ä½¿ç”¨æ­£å¼è¯æ±‡ (demonstrate, investigate, etc.)
    - ä½¿ç”¨å¤æ‚å¥å¼ (subordinate clauses)
    - é¿å…å£è¯­åŒ–è¡¨è¾¾
    """
    score = 5.0  # åŸºç¡€åˆ†

    # è¢«åŠ¨è¯­æ€
    if re.search(r'\b(is|are|was|were|been|being)\s+\w+ed\b', phrase):
        score += 1.5

    # æ­£å¼å­¦æœ¯è¯æ±‡
    academic_words = [
        'demonstrate', 'investigate', 'examine', 'analyze', 'evaluate',
        'illustrate', 'elucidate', 'substantiate', 'corroborate', 'methodology',
        'furthermore', 'moreover', 'consequently', 'notwithstanding'
    ]
    for word in academic_words:
        if word in phrase.lower():
            score += 0.5

    # å¤æ‚å¥å¼ (ä»å¥)
    if re.search(r'\b(which|that|who|whom|where|when|although|whereas)\b', phrase):
        score += 1.0

    # é™åˆ¶åœ¨ 0-10 èŒƒå›´
    return min(10.0, max(0.0, round(score, 1)))

def evaluate_frequency(phrase: str) -> str:
    """
    åŸºäºçŸ­è¯­ç‰¹å¾è¯„ä¼°ä½¿ç”¨é¢‘ç‡

    ç®€å•è§„åˆ™:
    - çŸ­ä¸”å¸¸è§ â†’ very_high
    - ä¸­ç­‰å¤æ‚åº¦ â†’ high
    - å¤æ‚æˆ–ä¸“ä¸š â†’ medium
    """
    if len(phrase) < 30 and re.search(r'\b(this|the|a|an|in|on|at)\b', phrase):
        return "very_high"
    elif len(phrase) < 60:
        return "high"
    else:
        return "medium"

def scrape_all_sections() -> Dict:
    """
    çˆ¬å–æ‰€æœ‰éƒ¨åˆ†å¹¶ç”Ÿæˆå®Œæ•´çš„ JSON æ•°æ®
    """
    phrasebank_data = {
        "name": "Manchester Academic Phrasebank",
        "version": "2.0.0",
        "source": "University of Manchester",
        "url": "https://www.phrasebank.manchester.ac.uk/",
        "license": "Educational Use",
        "totalPhrases": 0,
        "lastUpdated": time.strftime("%Y-%m-%d"),
        "sections": {},
        "citations": {
            "reporting_verbs_strong": [],
            "reporting_verbs_moderate": [],
            "reporting_verbs_neutral": []
        },
        "transitions": {
            "contrast": [],
            "addition": [],
            "result": [],
            "emphasis": []
        }
    }

    # çˆ¬å–å„ä¸ªéƒ¨åˆ†
    for section_name, url in SECTIONS.items():
        phrases = scrape_section(url, section_name)

        # æŒ‰ subsection ç»„ç»‡
        subsections = {}
        for phrase in phrases:
            subsection = phrase['subsection']
            if subsection not in subsections:
                subsections[subsection] = []
            subsections[subsection].append(phrase)

        phrasebank_data['sections'][section_name] = subsections
        phrasebank_data['totalPhrases'] += len(phrases)

        # å»¶è¿Ÿé¿å…è¢«å°
        time.sleep(2)

    return phrasebank_data

def save_to_json(data: Dict, output_file: str):
    """ä¿å­˜åˆ° JSON æ–‡ä»¶"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… Saved to {output_file}")
    print(f"ğŸ“Š Total phrases: {data['totalPhrases']}")

def main():
    print("ğŸš€ Manchester Academic Phrasebank Scraper")
    print("=" * 60)

    # çˆ¬å–æ‰€æœ‰æ•°æ®
    data = scrape_all_sections()

    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = "../data/manchester-phrasebank-full.json"
    save_to_json(data, output_file.replace('../', ''))

    print("\n" + "=" * 60)
    print("âœ… Scraping completed!")
    print(f"\nğŸ“ Output file: {output_file}")
    print(f"ğŸ“Š Total phrases: {data['totalPhrases']}")
    print("\nğŸ’¡ Next steps:")
    print("   1. Review the JSON file for quality")
    print("   2. Manually add 'usage' descriptions if needed")
    print("   3. Import to MyDictionary extension")

if __name__ == "__main__":
    main()
