#!/usr/bin/env python3
"""
Manchester Academic Phrasebank Scraper V2
æ”¹è¿›ç‰ˆ - æ­£ç¡®è§£æç½‘é¡µä¸­çš„å­¦æœ¯çŸ­è¯­
"""

import requests
from bs4 import BeautifulSoup
import json
import re
from typing import List, Dict
import time

BASE_URL = "https://www.phrasebank.manchester.ac.uk"

# è®ºæ–‡å„éƒ¨åˆ†çš„ URL
SECTIONS = {
    "introduction": f"{BASE_URL}/introducing-work/",
    "methods": f"{BASE_URL}/describing-methods/",
    "results": f"{BASE_URL}/reporting-results/",
    "discussion": f"{BASE_URL}/discussing-findings/",
    "conclusion": f"{BASE_URL}/writing-conclusions/",
}

def clean_phrase(phrase: str) -> str:
    """æ¸…ç†çŸ­è¯­æ–‡æœ¬"""
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    phrase = re.sub(r'\s+', ' ', phrase).strip()
    # ç§»é™¤HTMLå®ä½“
    phrase = phrase.replace('&nbsp;', ' ')
    return phrase

def evaluate_academic_score(phrase: str) -> float:
    """è¯„ä¼°å­¦æœ¯åº¦è¯„åˆ† (0-10)"""
    score = 5.0

    # è¢«åŠ¨è¯­æ€ +1.5
    if re.search(r'\b(is|are|was|were|been|being)\s+\w+ed\b', phrase, re.IGNORECASE):
        score += 1.5

    # å­¦æœ¯è¯æ±‡
    academic_words = [
        'demonstrate', 'investigate', 'examine', 'analyze', 'evaluate',
        'assess', 'determine', 'establish', 'identify', 'explore',
        'indicate', 'reveal', 'suggest', 'propose', 'argue',
        'hypothesis', 'objective', 'methodology', 'significant', 'substantial'
    ]
    for word in academic_words:
        if word in phrase.lower():
            score += 0.5

    # æ­£å¼è¿æ¥è¯
    formal_connectors = [
        'furthermore', 'moreover', 'consequently', 'therefore',
        'nevertheless', 'notwithstanding', 'thus', 'hence'
    ]
    for connector in formal_connectors:
        if connector in phrase.lower():
            score += 0.5

    # é™åˆ¶åœ¨ 0-10 èŒƒå›´å†…
    return min(10.0, max(0.0, round(score, 1)))

def evaluate_frequency(phrase: str) -> str:
    """è¯„ä¼°ä½¿ç”¨é¢‘ç‡"""
    # ç®€å•è§„åˆ™:çŸ­è¯­è¶ŠçŸ­,ä½¿ç”¨é¢‘ç‡è¶Šé«˜
    word_count = len(phrase.split())

    if word_count <= 5:
        return 'very_high'
    elif word_count <= 10:
        return 'high'
    else:
        return 'medium'

def scrape_section(url: str, section_name: str) -> Dict[str, List[Dict]]:
    """
    çˆ¬å–æŸä¸ªéƒ¨åˆ†çš„å­¦æœ¯çŸ­è¯­

    Returns:
        {subsection_name: [phrase_objects]}
    """
    print(f"ğŸ“– Scraping {section_name} from {url}...")

    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')

        subsections = {}
        phrase_id = 0

        # æŸ¥æ‰¾æ‰€æœ‰åŒ…å«çŸ­è¯­çš„æ®µè½
        # Manchester Phrasebank çš„çŸ­è¯­é€šå¸¸åœ¨ <p> æ ‡ç­¾ä¸­,ç”¨ <br> åˆ†éš”
        content_divs = soup.find_all(['div', 'section'], class_=re.compile('content|main|entry'))

        if not content_divs:
            # å¤‡ç”¨æ–¹æ¡ˆ:æŸ¥æ‰¾æ‰€æœ‰ p æ ‡ç­¾
            content_divs = [soup.find('body')]

        current_subsection = 'general'

        for div in content_divs:
            if not div:
                continue

            # æŸ¥æ‰¾æ ‡é¢˜ (h2, h3, h4) ä½œä¸ºå­åˆ†ç±»
            for heading in div.find_all(['h2', 'h3', 'h4']):
                heading_text = clean_phrase(heading.get_text())
                if heading_text and len(heading_text) > 3:
                    # è½¬æ¢ä¸ºåˆæ³•çš„ subsection åç§°
                    current_subsection = re.sub(r'[^a-z0-9]+', '_', heading_text.lower())

            # æŸ¥æ‰¾æ®µè½
            for p in div.find_all('p'):
                if not p:
                    continue

                # è·å–HTMLå†…å®¹ä»¥ä¿ç•™ <br> æ ‡ç­¾
                p_html = str(p)

                # æŒ‰ <br> åˆ†å‰²çŸ­è¯­
                phrases_raw = re.split(r'<br\s*/?>', p_html)

                for phrase_raw in phrases_raw:
                    # ç§»é™¤HTMLæ ‡ç­¾
                    phrase_text = BeautifulSoup(phrase_raw, 'html.parser').get_text()
                    phrase_text = clean_phrase(phrase_text)

                    # è¿‡æ»¤æ— æ•ˆçŸ­è¯­
                    if not phrase_text or len(phrase_text) < 10:
                        continue

                    # è¿‡æ»¤å¯¼èˆªå…ƒç´ 
                    if any(skip in phrase_text.lower() for skip in [
                        'contact us', 'find us', 'connect with',
                        'copyright', 'all rights', 'university of manchester',
                        'phrasebank', 'twitter', 'facebook'
                    ]):
                        continue

                    # ç¡®ä¿æ˜¯å®Œæ•´å¥å­ (åŒ…å«åŠ¨è¯æˆ–æœ‰æ„ä¹‰çš„å­¦æœ¯çŸ­è¯­)
                    if not re.search(r'\b(is|are|was|were|be|been|have|has|had|will|would|can|could|may|might|should|this|the|these|those|to|of|in|on|at)\b', phrase_text.lower()):
                        continue

                    phrase_id += 1

                    if current_subsection not in subsections:
                        subsections[current_subsection] = []

                    phrase_obj = {
                        'id': f"{section_name}_{current_subsection}_{phrase_id}",
                        'phrase': phrase_text,
                        'usage': '',  # æ‰‹åŠ¨è¡¥å……
                        'academicScore': evaluate_academic_score(phrase_text),
                        'frequency': evaluate_frequency(phrase_text),
                        'examples': [phrase_text],
                        'section': section_name,
                        'subsection': current_subsection
                    }

                    subsections[current_subsection].append(phrase_obj)

        total_phrases = sum(len(phrases) for phrases in subsections.values())
        print(f"  âœ… Found {total_phrases} phrases in {len(subsections)} subsections")

        # æ˜¾ç¤ºæ¯ä¸ªå­åˆ†ç±»çš„æ•°é‡
        for subsection, phrases in subsections.items():
            if phrases:
                print(f"     ğŸ“‚ {subsection}: {len(phrases)} phrases")

        return subsections

    except Exception as e:
        print(f"  âŒ Error scraping {section_name}: {e}")
        return {}

def scrape_all_sections() -> Dict:
    """çˆ¬å–æ‰€æœ‰éƒ¨åˆ†"""
    data = {
        'sections': {},
        'citations': {},
        'transitions': {},
        'metadata': {
            'source': 'Manchester Academic Phrasebank',
            'url': BASE_URL,
            'scrapedAt': time.strftime('%Y-%m-%d %H:%M:%S'),
            'version': '2.0'
        }
    }

    for section_name, url in SECTIONS.items():
        subsections = scrape_section(url, section_name)
        if subsections:
            data['sections'][section_name] = subsections

        # ç¤¼è²Œå»¶è¿Ÿ
        time.sleep(1)

    return data

def save_to_json(data: Dict, output_file: str):
    """ä¿å­˜æ•°æ®åˆ° JSON æ–‡ä»¶"""
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    # ç»Ÿè®¡
    total_phrases = sum(
        len(phrases)
        for section in data['sections'].values()
        for phrases in section.values()
    )

    print(f"\nâœ… Saved to {output_file}")
    print(f"ğŸ“Š Total phrases: {total_phrases}")

def main():
    print("ğŸš€ Manchester Academic Phrasebank Scraper V2")
    print("=" * 60)

    # çˆ¬å–æ‰€æœ‰éƒ¨åˆ†
    data = scrape_all_sections()

    # ä¿å­˜åˆ°æ–‡ä»¶
    output_file = "data/manchester-phrasebank-full.json"
    save_to_json(data, output_file)

    print("\n" + "=" * 60)
    print("âœ… Scraping completed!")
    print(f"\nğŸ“ Output file: {output_file}")

    # ç»Ÿè®¡
    total_phrases = sum(
        len(phrases)
        for section in data['sections'].values()
        for phrases in section.values()
    )
    print(f"ğŸ“Š Total phrases: {total_phrases}")

    print("\nğŸ’¡ Next steps:")
    print("   1. Review the JSON file for quality")
    print("   2. Manually add 'usage' descriptions if needed")
    print("   3. Replace academic-phrasebank.json with this file")

if __name__ == '__main__':
    main()
